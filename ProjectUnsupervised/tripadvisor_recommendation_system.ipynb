{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TripAdvisor Recommendation System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements and evaluates different recommendation systems for hotel reviews.\n",
    "It compares traditional information retrieval methods (BM25) with modern transformer-based approaches.\n",
    "The goal is to find hotels with similar characteristics based on review text and ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexs/PycharmProjects/MachineLearningForNLP/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK resources and implement device handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device =  torch.device('mps') if torch.backends.mps.is_available() else device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Loading and Preprocessing\n",
    "\n",
    "1. Load TripAdvisor reviews from CSV\n",
    "2. Filter reviews to ensure all required rating aspects are present\n",
    "3. Group reviews by hotel (offering_id)\n",
    "4. Calculate average ratings for each aspect\n",
    "5. Prepare text data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (878561, 10)\n",
      "DataFrame shape after filtering: (436391, 10)\n",
      "   offering_id   service  cleanliness   overall     value  location  \\\n",
      "0        72572  4.601010     4.636364  4.388889  4.323232  4.570707   \n",
      "1        72579  4.232000     4.240000  3.888000  4.152000  4.192000   \n",
      "2        72586  4.250000     4.287879  4.045455  4.053030  4.537879   \n",
      "3        72598  3.243243     3.243243  2.918919  3.054054  3.027027   \n",
      "4        73236  4.277778     3.111111  3.388889  3.777778  4.111111   \n",
      "\n",
      "   sleep_quality     rooms                                            reviews  \n",
      "0       4.333333  4.282828  I had to make fast visit to seattle and I foun...  \n",
      "1       3.768000  3.856000  Great service, rooms were clean, could use som...  \n",
      "2       4.113636  3.992424  Beautiful views of the space needle - especial...  \n",
      "3       3.270270  3.189189  This hotel is in need of some serious updates....  \n",
      "4       3.722222  3.222222  My experience at this days inn was perfect. th...  \n",
      "\n",
      "DataFrame shape: (3754, 9)\n",
      "\n",
      "Column names: ['offering_id', 'service', 'cleanliness', 'overall', 'value', 'location', 'sleep_quality', 'rooms', 'reviews']\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "print(f\"Original DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Convert the 'ratings' column from string to dictionary\n",
    "df['ratings'] = df['ratings'].apply(ast.literal_eval)\n",
    "\n",
    "# Define required aspects\n",
    "required_aspects = [\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"]\n",
    "\n",
    "# Filter rows with at least the required aspects\n",
    "df_filtered = df[df['ratings'].apply(lambda x: all(aspect in x for aspect in required_aspects))]\n",
    "\n",
    "print(f\"DataFrame shape after filtering: {df_filtered.shape}\")\n",
    "\n",
    "if df_filtered.empty:\n",
    "    print(\"No reviews found with all required aspects. Printing unique aspects found in the dataset:\")\n",
    "    all_aspects = set()\n",
    "    for rating in df['ratings']:\n",
    "        all_aspects.update(rating.keys())\n",
    "    print(sorted(all_aspects))\n",
    "    data = pd.DataFrame(columns=['offering_id'] + required_aspects + ['reviews'])\n",
    "else:\n",
    "    # Group by offering_id\n",
    "    data = df_filtered.groupby('offering_id').agg({\n",
    "        'text': ' '.join,  \n",
    "        'ratings': list  \n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate average ratings for each aspect\n",
    "    for aspect in required_aspects:\n",
    "        data[aspect] = data['ratings'].apply(lambda x: np.mean([review.get(aspect, np.nan) for review in x]))\n",
    "\n",
    "    # Rename 'text' column to 'reviews'\n",
    "    data = data.rename(columns={'text': 'reviews'})\n",
    "\n",
    "    # Select and order the final columns\n",
    "    final_columns = ['offering_id'] + required_aspects + ['reviews']\n",
    "    data = data[final_columns]\n",
    "\n",
    "print(data.head())\n",
    "print(\"\\nDataFrame shape:\", data.shape)\n",
    "\n",
    "print(\"\\nColumn names:\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text Preprocessing Function\n",
    "\n",
    "Comprehensive text cleaning pipeline that:\n",
    "1. Converts to lowercase\n",
    "2. Removes URLs, email addresses, numbers, and special characters\n",
    "3. Tokenizes text and removes stopwords\n",
    "4. Removes short words and applies lemmatization\n",
    "5. Removes domain-specific stopwords (hotel-related common terms)\n",
    "6. Ensures consistent formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Enhanced text preprocessing function with multiple cleaning steps\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and handle basic cleaning\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove numbers and special characters, keeping only letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Remove short words (length < 4)\n",
    "    tokens = [token for token in tokens if len(token) > 3]\n",
    "    \n",
    "    # Lemmatization (convert words to their base form)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Remove common hotel-related words that might not add value\n",
    "    hotel_stopwords = {\n",
    "        'hotel', 'room', 'stay', 'stayed', 'night', 'day', \n",
    "        'would', 'could', 'really', 'get', 'got', 'one',\n",
    "        'also', 'us', 'back', 'even', 'well'\n",
    "    }\n",
    "    tokens = [token for token in tokens if token not in hotel_stopwords]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    processed_text = ' '.join(processed_text.split())\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "data['processed_reviews'] = data['reviews'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate word counts to understrand the impact of the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews Statistics:\n",
      "Average words per hotel: 16916.49\n",
      "Median words per hotel: 5509.50\n",
      "Min words: 1\n",
      "Max words: 294452\n",
      "\n",
      "Percentiles:\n",
      "count      3754.000000\n",
      "mean      16916.485882\n",
      "std       28293.752907\n",
      "min           1.000000\n",
      "25%        1682.750000\n",
      "50%        5509.500000\n",
      "75%       20139.500000\n",
      "max      294452.000000\n",
      "Name: reviews, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processed Reviews Statistics:\n",
      "Average words per hotel: 6976.72\n",
      "Median words per hotel: 2232.00\n",
      "Min words: 0\n",
      "Max words: 124229\n",
      "\n",
      "Percentiles:\n",
      "count      3754.000000\n",
      "mean       6976.724294\n",
      "std       11747.965301\n",
      "min           0.000000\n",
      "25%         674.250000\n",
      "50%        2232.000000\n",
      "75%        8372.500000\n",
      "max      124229.000000\n",
      "Name: processed_reviews, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate word counts\n",
    "reviews_word_counts = data['reviews'].str.split().str.len()\n",
    "processed_word_counts = data['processed_reviews'].str.split().str.len()\n",
    "\n",
    "print(\"Reviews Statistics:\")\n",
    "print(f\"Average words per hotel: {reviews_word_counts.mean():.2f}\")\n",
    "print(f\"Median words per hotel: {reviews_word_counts.median():.2f}\")\n",
    "print(f\"Min words: {reviews_word_counts.min()}\")\n",
    "print(f\"Max words: {reviews_word_counts.max()}\")\n",
    "print(f\"\\nPercentiles:\")\n",
    "print(reviews_word_counts.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Processed Reviews Statistics:\")\n",
    "print(f\"Average words per hotel: {processed_word_counts.mean():.2f}\")\n",
    "print(f\"Median words per hotel: {processed_word_counts.median():.2f}\")\n",
    "print(f\"Min words: {processed_word_counts.min()}\")\n",
    "print(f\"Max words: {processed_word_counts.max()}\")\n",
    "print(f\"\\nPercentiles:\")\n",
    "print(processed_word_counts.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation Functions:\n",
    "Two main evaluation functions:\n",
    "1. evaluate_model: General evaluation for transformer-based models\n",
    "2. evaluate_model_bm25: Specific evaluation for BM25 models\n",
    "   \n",
    "Both calculate:\n",
    "- Mean Squared Error (MSE) between predicted and actual ratings\n",
    "- Processing time metrics\n",
    "- Success rate of query evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, query_data, full_data):\n",
    "\n",
    "    mse_scores = []\n",
    "    aspects = [\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"]\n",
    "    total_scoring_time = 0\n",
    "    \n",
    "    query_doc_ids = {idx: query['offering_id'] for idx, query in query_data.iterrows()}\n",
    "    \n",
    "    for idx, query in tqdm(query_data.iterrows(), total=len(query_data), desc=\"Evaluating queries\"):\n",
    "        try:\n",
    "            # Time the scoring\n",
    "            start_time = time.time()\n",
    "            scores = model(query['processed_reviews'])\n",
    "            scoring_time = time.time() - start_time\n",
    "            total_scoring_time += scoring_time\n",
    "            \n",
    "            # Get indices of top 2 scores\n",
    "            top_2_indices = np.argpartition(scores, -2)[-2:]\n",
    "            top_2_indices = top_2_indices[np.argsort(scores[top_2_indices])][::-1]  \n",
    "            \n",
    "            # If the first best match is the query document itself, use the second best\n",
    "            if full_data.iloc[top_2_indices[0]]['offering_id'] == query_doc_ids[idx]:\n",
    "                best_index = top_2_indices[1]\n",
    "            else:\n",
    "                best_index = top_2_indices[0]\n",
    "            \n",
    "            # Calculate MSE\n",
    "            query_ratings = query[aspects].values\n",
    "            best_doc_ratings = full_data.iloc[best_index][aspects].values\n",
    "            mse = mean_squared_error(query_ratings, best_doc_ratings)\n",
    "            mse_scores.append(mse)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {idx}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Print summary statistics\n",
    "    avg_scoring_time = total_scoring_time / len(query_data) if query_data.shape[0] > 0 else 0\n",
    "    print(f\"\\nAverage scoring time per query: {avg_scoring_time:.4f} seconds\")\n",
    "    print(f\"Total scoring time: {total_scoring_time:.4f} seconds\")\n",
    "    print(f\"Successfully evaluated queries: {len(mse_scores)}/{len(query_data)}\")\n",
    "    \n",
    "    if not mse_scores:\n",
    "        print(\"Warning: No valid evaluations were performed\")\n",
    "        return float('inf')\n",
    "    \n",
    "    avg_mse = np.mean(mse_scores)\n",
    "    print(f\"MSE Average: {avg_mse:.4f}\")\n",
    "    \n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_bm25(model, query_data, full_data, column_name='processed_reviews'):\n",
    "\n",
    "    mse_scores = []\n",
    "    aspects = [\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"]\n",
    "    total_scoring_time = 0\n",
    "    \n",
    "    query_doc_ids = {idx: query['offering_id'] for idx, query in query_data.iterrows()}\n",
    "    \n",
    "    for idx, query in tqdm(query_data.iterrows(), total=len(query_data), desc=\"Evaluating queries\"):\n",
    "        try:\n",
    "            query_tokens = query[column_name].split()\n",
    "            \n",
    "            # Time the scoring\n",
    "            start_time = time.time()\n",
    "            \n",
    "            scores = model.get_scores(query_tokens)\n",
    "            \n",
    "            scoring_time = time.time() - start_time\n",
    "            total_scoring_time += scoring_time\n",
    "            \n",
    "            # Get indices of top 2 scores\n",
    "            top_2_indices = np.argpartition(scores, -2)[-2:]\n",
    "            top_2_indices = top_2_indices[np.argsort(scores[top_2_indices])][::-1]\n",
    "            \n",
    "            # If the first best match is the query document itself, use the second best\n",
    "            if full_data.iloc[top_2_indices[0]]['offering_id'] == query_doc_ids[idx]:\n",
    "                best_index = top_2_indices[1]\n",
    "            else:\n",
    "                best_index = top_2_indices[0]\n",
    "            \n",
    "            # Calculate MSE\n",
    "            query_ratings = query[aspects].values\n",
    "            best_doc_ratings = full_data.iloc[best_index][aspects].values\n",
    "            mse = mean_squared_error(query_ratings, best_doc_ratings)\n",
    "            mse_scores.append(mse)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {idx}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Print summary statistics\n",
    "    avg_scoring_time = total_scoring_time / len(query_data) if query_data.shape[0] > 0 else 0\n",
    "    print(f\"\\nAverage scoring time per query: {avg_scoring_time:.4f} seconds\")\n",
    "    print(f\"Total scoring time: {total_scoring_time:.4f} seconds\")\n",
    "    print(f\"Successfully evaluated queries: {len(mse_scores)}/{len(query_data)}\")\n",
    "    \n",
    "    if not mse_scores:\n",
    "        print(\"Warning: No valid evaluations were performed\")\n",
    "        return float('inf')\n",
    "    \n",
    "    avg_mse = np.mean(mse_scores)\n",
    "    print(f\"MSE Average: {avg_mse:.4f}\")\n",
    "    \n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_bm25_model:\n",
    "- Creates traditional BM25 retrieval model\n",
    "- Uses exact keyword matching with TF-IDF principles\n",
    "\n",
    "create_hybrid_model_with_dense_retriever:\n",
    "- Combines BM25 with neural embeddings\n",
    "- Balances keyword matching with semantic understanding\n",
    "\n",
    "create_mpnet_domain_model:\n",
    "- Implements MPNet transformer architecture\n",
    "- Optimized for domain-specific semantic similarity\n",
    "\n",
    "create_simcse_model:\n",
    "- Uses contrastive learning approach\n",
    "- Generates robust sentence embeddings\n",
    "\n",
    "create_mxbai_embed_model:\n",
    "- Implements MXBAI's embedding model\n",
    "- Optimized for efficient retrieval\n",
    "\n",
    "create_mxbai_colbert_model:\n",
    "- Implements MXBAI's ColBERT architecture\n",
    "- Uses late interaction for precise matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_bm25_model(corpus):\n",
    "    try:\n",
    "        tokenized_corpus = [doc.split(' ') for doc in corpus if isinstance(doc, str)]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "        return bm25\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating BM25 model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_hybrid_model_with_dense_retriever(corpus, alpha=0.6, beta=0.4):\n",
    "    \"\"\"\n",
    "    Creates a hybrid model combining BM25 and SentenceTransformer with GPU support\n",
    "    \"\"\"\n",
    "    tokenized_corpus = [doc.split() for doc in corpus if isinstance(doc, str)]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    dense_model = SentenceTransformer('all-mpnet-base-v2').to(device)\n",
    "    \n",
    "    # Pre-compute document embeddings\n",
    "    doc_embeddings = dense_model.encode(corpus, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    def get_hybrid_scores(query_text):\n",
    "        try:\n",
    "            # 1. Get BM25 scores\n",
    "            query_tokens = query_text.split()\n",
    "            bm25_scores = np.array(bm25.get_scores(query_tokens))\n",
    "                    \n",
    "            # 2. Get dense retriever scores\n",
    "            query_embedding = dense_model.encode(query_text, convert_to_tensor=True, device=device)\n",
    "            \n",
    "            # Calculate cosine similarity on GPU\n",
    "            dense_scores = torch.nn.functional.cosine_similarity(\n",
    "                query_embedding.unsqueeze(0),\n",
    "                doc_embeddings\n",
    "            ).cpu().numpy()\n",
    "              \n",
    "            # Combine scores\n",
    "            final_scores = alpha * bm25_scores + beta * dense_scores\n",
    "            \n",
    "            return final_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hybrid scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_hybrid_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mpnet_domain_model(corpus, batch_size=32):\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Encode with special handling for reviews\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            query_embedding = query_embedding.cpu()\n",
    "            \n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            \n",
    "            return similarities.numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simcse_model(corpus, batch_size=32):\n",
    "\n",
    "    model = SentenceTransformer('princeton-nlp/unsup-simcse-bert-base-uncased').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Get embeddings with temperature scaling\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Get query embedding with same temperature scaling\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            query_embedding = query_embedding.cpu()\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            scores = similarities.numpy()\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mxbai_embed_model(corpus, batch_size=32):\n",
    "\n",
    "    model = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Add instruction prefix for better retrieval performance\n",
    "        batch_texts = [f\"Represent this sentence for searching relevant passages: {text}\" for text in batch_texts]\n",
    "        \n",
    "        # Get embeddings\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Add instruction prefix for query\n",
    "            query_text = f\"Represent this sentence for searching relevant passages: {query_text}\"\n",
    "            \n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            query_embedding = query_embedding.cpu()\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            scores = similarities.numpy()\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mxbai-embed scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mxbai_colbert_model(corpus, batch_size=16):\n",
    "\n",
    "    model_name = 'mixedbread-ai/mxbai-colbert-large-v1'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Add instruction prefix for better retrieval\n",
    "        batch_texts = [f\"Represent this document for retrieval: {text}\" for text in batch_texts]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            attention_mask = encoded['attention_mask'].unsqueeze(-1)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            sentence_embeddings = (token_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            doc_embeddings.append(sentence_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Add instruction prefix\n",
    "            query_text = f\"Represent this query for retrieval: {query_text}\"\n",
    "            \n",
    "            # Encode query\n",
    "            encoded = tokenizer(\n",
    "                query_text,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get query embedding\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**encoded)\n",
    "                attention_mask = encoded['attention_mask'].unsqueeze(-1)\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "                query_embedding = (token_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "                query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "            \n",
    "            # Move to CPU and compute similarity\n",
    "            query_embedding = query_embedding.cpu()\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding.T)\n",
    "            scores = similarities.squeeze().numpy()\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            temperature = 0.05\n",
    "            scores = np.exp(scores / temperature)\n",
    "            scores = scores / np.sum(scores)\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mxbai-colbert scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Section\n",
    "\n",
    "Tests each model implementation with:\n",
    "- 100 random sample queries\n",
    "- Both preprocessed and raw text versions for bm25\n",
    "- Comprehensive performance metrics:\n",
    "  * MSE (accuracy)\n",
    "  * Processing time\n",
    "  * Query success rate\n",
    "Models evaluated:\n",
    "1. BM25 (with and without preprocessing)\n",
    "2. Hybrid BM25 + Dense Retriever\n",
    "3. MPNet\n",
    "4. SimCSE\n",
    "5. MXBAI Embed\n",
    "6. MXBAI ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of data for querying (e.g., 100 random samples)\n",
    "query_data = data.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 model without preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 100/100 [17:35<00:00, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scoring time per query: 10.5526 seconds\n",
      "Total scoring time: 1055.2592 seconds\n",
      "Successfully evaluated queries: 100/100\n",
      "MSE Average: 0.5476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating BM25 model without preprocessing...\")\n",
    "bm25_model = create_bm25_model(data['reviews'].tolist())\n",
    "bm25_scores = evaluate_model_bm25(bm25_model, query_data, data, 'reviews')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 model with preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 100/100 [06:13<00:00,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scoring time per query: 3.7323 seconds\n",
      "Total scoring time: 373.2272 seconds\n",
      "Successfully evaluated queries: 100/100\n",
      "MSE Average: 0.4910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating BM25 model with preprocessing...\")\n",
    "bm25_model = create_bm25_model(data['processed_reviews'].tolist())\n",
    "bm25_scores = evaluate_model_bm25(bm25_model, query_data, data, 'processed_reviews')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Hybrid model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 100/100 [06:41<00:00,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scoring time per query: 4.0119 seconds\n",
      "Total scoring time: 401.1897 seconds\n",
      "Successfully evaluated queries: 100/100\n",
      "MSE Average: 0.4910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluating Hybrid model...\")\n",
    "hybrid_model = create_hybrid_model_with_dense_retriever(data['processed_reviews'].tolist())\n",
    "hybrid_scores = evaluate_model(hybrid_model, query_data, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MPNet model...\n",
      "Computing document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 118/118 [02:02<00:00,  1.04s/it]\n",
      "Evaluating queries: 100%|██████████| 100/100 [00:05<00:00, 19.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scoring time per query: 0.0509 seconds\n",
      "Total scoring time: 5.0927 seconds\n",
      "Successfully evaluated queries: 100/100\n",
      "MSE Average: 0.4637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluating MPNet model...\")\n",
    "mpnet_model = create_mpnet_domain_model(data['processed_reviews'].tolist())\n",
    "mpnet_scores = evaluate_model(mpnet_model, query_data, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SimCSE model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name princeton-nlp/unsup-simcse-bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7792e9aa84e47179c42341e48f4b013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  50%|#####     | 220M/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7365fac95ef44e6eacd6ec238b29c46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b24bdec4c2428f91f243237bf1126e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2174906e9d514bee8c6b2387900b709a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 118/118 [02:55<00:00,  1.49s/it]\n",
      "Evaluating queries: 100%|██████████| 100/100 [00:06<00:00, 15.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scoring time per query: 0.0616 seconds\n",
      "Total scoring time: 6.1583 seconds\n",
      "Successfully evaluated queries: 100/100\n",
      "MSE Average: 0.4162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluating SimCSE model...\")\n",
    "simcse_model = create_simcse_model(data['processed_reviews'].tolist())\n",
    "simcse_scores = evaluate_model(simcse_model, query_data, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MXBAI Embed model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8787173b0d0342ad848250d8d8d84501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb856d8739f484690799c3001cc114b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bdbf69e10e47edaabaf8ab1a5eae77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/114k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841fd1b41ab64e5cb08f0bbe2df0ca04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820f590a9687494bbc30811008804199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02eecce7895b4ae381ef04f515efd7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fda79cb9c9a4a0897694efd8550eb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd7bfa3f8904a7a8d001de4ea9fd638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e0575f215a494ba02cc1d3e0d23001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb75ccfc9e5f4589a7e4a0f46c3c0e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f603e35aee4f414b840f2bad216d3be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 118/118 [08:35<00:00,  4.37s/it]\n",
      "Evaluating queries: 100%|██████████| 100/100 [00:13<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scoring time per query: 0.1343 seconds\n",
      "Total scoring time: 13.4292 seconds\n",
      "Successfully evaluated queries: 100/100\n",
      "MSE Average: 0.5167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluating MXBAI Embed model...\")\n",
    "mxbai_embed_model = create_mxbai_embed_model(data['processed_reviews'].tolist())\n",
    "mxbai_embed_scores = evaluate_model(mxbai_embed_model, query_data, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MXBAI ColBERT model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61a0978262a4e05bcb214a7cdc00db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cadbfda983f444d9f417ac499f50509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d10317cfa6a48b2964fbe17af746985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1629d423d8bb45f2b27a5b5923594cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f3480b66d64e9cbd560c8c0330821b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe93803750fd481491e33e90de21fa40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/671M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 235/235 [08:12<00:00,  2.09s/it]\n",
      "Evaluating queries: 100%|██████████| 100/100 [00:13<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scoring time per query: 0.1303 seconds\n",
      "Total scoring time: 13.0313 seconds\n",
      "Successfully evaluated queries: 100/100\n",
      "MSE Average: 0.4954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evaluating MXBAI ColBERT model...\")\n",
    "mxbai_colbert_model = create_mxbai_colbert_model(data['processed_reviews'].tolist())\n",
    "mxbai_colbert_scores = evaluate_model(mxbai_colbert_model, query_data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
