{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TripAdvisor Recommendation System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (878561, 10)\n",
      "DataFrame shape after filtering: (436391, 10)\n",
      "   offering_id   service  cleanliness   overall     value  location  \\\n",
      "0        72572  4.601010     4.636364  4.388889  4.323232  4.570707   \n",
      "1        72579  4.232000     4.240000  3.888000  4.152000  4.192000   \n",
      "2        72586  4.250000     4.287879  4.045455  4.053030  4.537879   \n",
      "3        72598  3.243243     3.243243  2.918919  3.054054  3.027027   \n",
      "4        73236  4.277778     3.111111  3.388889  3.777778  4.111111   \n",
      "\n",
      "   sleep_quality     rooms                                            reviews  \n",
      "0       4.333333  4.282828  I had to make fast visit to seattle and I foun...  \n",
      "1       3.768000  3.856000  Great service, rooms were clean, could use som...  \n",
      "2       4.113636  3.992424  Beautiful views of the space needle - especial...  \n",
      "3       3.270270  3.189189  This hotel is in need of some serious updates....  \n",
      "4       3.722222  3.222222  My experience at this days inn was perfect. th...  \n",
      "\n",
      "DataFrame shape: (3754, 9)\n",
      "\n",
      "Column names: ['offering_id', 'service', 'cleanliness', 'overall', 'value', 'location', 'sleep_quality', 'rooms', 'reviews']\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "print(f\"Original DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Convert the 'ratings' column from string to dictionary\n",
    "df['ratings'] = df['ratings'].apply(ast.literal_eval)\n",
    "\n",
    "# Define required aspects\n",
    "required_aspects = [\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"]\n",
    "\n",
    "# Filter rows with at least the required aspects\n",
    "df_filtered = df[df['ratings'].apply(lambda x: all(aspect in x for aspect in required_aspects))]\n",
    "\n",
    "print(f\"DataFrame shape after filtering: {df_filtered.shape}\")\n",
    "\n",
    "if df_filtered.empty:\n",
    "    print(\"No reviews found with all required aspects. Printing unique aspects found in the dataset:\")\n",
    "    all_aspects = set()\n",
    "    for rating in df['ratings']:\n",
    "        all_aspects.update(rating.keys())\n",
    "    print(sorted(all_aspects))\n",
    "    data = pd.DataFrame(columns=['offering_id'] + required_aspects + ['reviews'])\n",
    "else:\n",
    "    # Group by offering_id\n",
    "    data = df_filtered.groupby('offering_id').agg({\n",
    "        'text': ' '.join,  # Concatenate all reviews\n",
    "        'ratings': list  # Keep all ratings\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate average ratings for each aspect\n",
    "    for aspect in required_aspects:\n",
    "        data[aspect] = data['ratings'].apply(lambda x: np.mean([review.get(aspect, np.nan) for review in x]))\n",
    "\n",
    "    # Rename 'text' column to 'reviews'\n",
    "    data = data.rename(columns={'text': 'reviews'})\n",
    "\n",
    "    # Select and order the final columns\n",
    "    final_columns = ['offering_id'] + required_aspects + ['reviews']\n",
    "    data = data[final_columns]\n",
    "\n",
    "# Print the first few rows and shape of the processed data\n",
    "print(data.head())\n",
    "print(\"\\nDataFrame shape:\", data.shape)\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"\\nColumn names:\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply text preprocessing to reviews\n",
    "data['processed_review'] = data['reviews'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, query_data, full_data, model_type='hybrid', k=5):\n",
    "    \"\"\"\n",
    "    Improved evaluation function for recommendation models\n",
    "    Args:\n",
    "        model: The model to evaluate (BM25 or hybrid)\n",
    "        query_data: DataFrame containing query samples\n",
    "        full_data: Complete DataFrame with all hotels\n",
    "        model_type: 'bm25' or 'hybrid'\n",
    "        k: Number of similar hotels to consider\n",
    "    Returns:\n",
    "        float: Average MSE across all queries\n",
    "    \"\"\"\n",
    "    mse_scores = []\n",
    "    aspects = [\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"]\n",
    "    \n",
    "    for idx, query in query_data.iterrows():\n",
    "        try:\n",
    "            # Get similarity scores based on model type\n",
    "            if model_type == 'bm25':\n",
    "                tokenized_query = query['processed_review'].split()\n",
    "                scores = model.get_scores(tokenized_query)\n",
    "            else:  # hybrid model\n",
    "                scores = model(query['processed_review'])\n",
    "            \n",
    "            # Create mask to exclude the query hotel itself\n",
    "            mask = np.ones(len(scores), dtype=bool)\n",
    "            query_hotel_id = query['offering_id']\n",
    "            mask[full_data['offering_id'] == query_hotel_id] = False\n",
    "            \n",
    "            # Apply mask and get top-k indices\n",
    "            masked_scores = scores[mask]\n",
    "            masked_indices = np.argsort(masked_scores)[-k:]\n",
    "            \n",
    "            # Map masked indices back to original indices\n",
    "            top_k_idx = np.where(mask)[0][masked_indices]\n",
    "            \n",
    "            # Calculate ratings\n",
    "            similar_ratings = []\n",
    "            query_ratings = []\n",
    "            \n",
    "            for aspect in aspects:\n",
    "                # Get average rating for similar hotels\n",
    "                avg_rating = full_data.iloc[top_k_idx][aspect].mean()\n",
    "                similar_ratings.append(avg_rating)\n",
    "                query_ratings.append(query[aspect])\n",
    "            \n",
    "            # Calculate MSE\n",
    "            mse = mean_squared_error(query_ratings, similar_ratings)\n",
    "            mse_scores.append(mse)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Return average MSE\n",
    "    if not mse_scores:\n",
    "        print(\"Warning: No valid evaluations were performed\")\n",
    "        return float('inf')\n",
    "    \n",
    "    avg_mse = np.mean(mse_scores)\n",
    "    std_mse = np.std(mse_scores)\n",
    "    print(f\"Number of evaluated queries: {len(mse_scores)}\")\n",
    "    print(f\"MSE Average for {model_type} model: {avg_mse:.4f}\")\n",
    "    print(f\"MSE Standard Deviation for {model_type} model: {std_mse:.4f}\")\n",
    "    \n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition and eval function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_bm25_model(corpus):\n",
    "    try:\n",
    "        tokenized_corpus = [doc.split() for doc in corpus if isinstance(doc, str)]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "        return bm25\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating BM25 model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_model_with_dense_retriever(corpus, alpha=0.85, beta=0.15):\n",
    "    \"\"\"\n",
    "    Creates a hybrid model combining BM25 and SentenceTransformer (all-mpnet-base-v2)\n",
    "    with proper tensor handling for Apple Silicon\n",
    "    \"\"\"\n",
    "    # Initialize models\n",
    "    tokenized_corpus = [doc.split() for doc in corpus if isinstance(doc, str)]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    dense_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    \n",
    "    # Pre-compute document embeddings and move to CPU\n",
    "    doc_embeddings = dense_model.encode(corpus, convert_to_tensor=True)\n",
    "    doc_embeddings = doc_embeddings.cpu()  # Move to CPU\n",
    "    \n",
    "    def get_hybrid_scores(query_text):\n",
    "        try:\n",
    "            # 1. Get BM25 scores\n",
    "            query_tokens = query_text.split()\n",
    "            bm25_scores = np.array(bm25.get_scores(query_tokens))\n",
    "                    \n",
    "            # 2. Get dense retriever scores\n",
    "            query_embedding = dense_model.encode(query_text, convert_to_tensor=True)\n",
    "            query_embedding = query_embedding.cpu()  # Move to CPU\n",
    "            \n",
    "            # Reshape embeddings for cosine similarity\n",
    "            query_embedding_reshaped = query_embedding.reshape(1, -1)\n",
    "            doc_embeddings_reshaped = doc_embeddings.reshape(len(corpus), -1)\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            dense_scores = cosine_similarity(query_embedding_reshaped, doc_embeddings_reshaped)[0]\n",
    "              \n",
    "            # Combine scores\n",
    "            final_scores = alpha * bm25_scores + beta * dense_scores\n",
    "            \n",
    "            return final_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hybrid scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_hybrid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_cross_encoder_hybrid_model(corpus, top_k=100):\n",
    "    \"\"\"\n",
    "    Creates a hybrid model using BM25 for initial retrieval and Cross-Encoder for re-ranking\n",
    "    \"\"\"\n",
    "    # Initialize BM25\n",
    "    tokenized_corpus = [doc.split() for doc in corpus if isinstance(doc, str)]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    # Initialize Cross-Encoder\n",
    "    model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    cross_encoder = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def get_hybrid_scores(query_text):\n",
    "        try:\n",
    "            # 1. Get initial candidates using BM25\n",
    "            query_tokens = query_text.split()\n",
    "            bm25_scores = np.array(bm25.get_scores(query_tokens))\n",
    "            top_k_idx = np.argsort(bm25_scores)[-top_k:]\n",
    "            \n",
    "            # 2. Prepare pairs for cross-encoder\n",
    "            pairs = []\n",
    "            for idx in top_k_idx:\n",
    "                pairs.append([query_text, corpus[idx]])\n",
    "            \n",
    "            # 3. Cross-encoder scoring\n",
    "            features = tokenizer(\n",
    "                pairs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                scores = cross_encoder(**features)\n",
    "                scores = torch.sigmoid(scores.logits).squeeze().numpy()\n",
    "            \n",
    "            # 4. Create final scores array\n",
    "            final_scores = np.zeros(len(corpus))\n",
    "            final_scores[top_k_idx] = scores\n",
    "            \n",
    "            return final_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hybrid scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_hybrid_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_colbert_model(corpus, max_length=128):\n",
    "    \"\"\"\n",
    "    Creates a ColBERT-style model that performs better than standard dense retrievers\n",
    "    by using contextual late interaction\n",
    "    \"\"\"\n",
    "    # Initialize BERT model and tokenizer\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Pre-compute document embeddings\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for doc in corpus:\n",
    "        # Tokenize document\n",
    "        tokens = tokenizer(\n",
    "            doc,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            embeddings = outputs.last_hidden_state.squeeze()  # [seq_len, hidden_dim]\n",
    "            doc_embeddings.append(embeddings)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Tokenize query\n",
    "            query_tokens = tokenizer(\n",
    "                query_text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Get query embeddings\n",
    "            with torch.no_grad():\n",
    "                query_outputs = model(**query_tokens)\n",
    "                query_embeddings = query_outputs.last_hidden_state.squeeze()  # [seq_len, hidden_dim]\n",
    "            \n",
    "            # Calculate MaxSim scores for each document\n",
    "            scores = []\n",
    "            for doc_emb in doc_embeddings:\n",
    "                # Calculate similarity matrix between query and document tokens\n",
    "                sim_matrix = torch.matmul(query_embeddings, doc_emb.T)  # [query_len, doc_len]\n",
    "                \n",
    "                # Max-pool over document dimension\n",
    "                max_sim = torch.max(sim_matrix, dim=1)[0]  # [query_len]\n",
    "                \n",
    "                # Sum over query tokens (with optional masking of padding)\n",
    "                score = torch.mean(max_sim).item()\n",
    "                scores.append(score)\n",
    "            \n",
    "            return np.array(scores)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ColBERT scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_biencoder_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a bi-encoder model using BERT embeddings for efficient semantic search\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of text documents\n",
    "        batch_size: Batch size for processing documents\n",
    "    Returns:\n",
    "        scoring function that computes similarity between query and documents\n",
    "    \"\"\"\n",
    "    # Initialize BERT model and tokenizer\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'  # Lightweight but effective model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Pre-compute document embeddings in batches\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(corpus), batch_size):\n",
    "        batch = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize and encode batch\n",
    "        encoded = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get BERT embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # Use [CLS] token embedding as document representation\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            doc_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Encode query\n",
    "            query_encoded = tokenizer(\n",
    "                query_text,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Get query embedding\n",
    "            with torch.no_grad():\n",
    "                query_outputs = model(**query_encoded)\n",
    "                query_embedding = query_outputs.last_hidden_state[:, 0, :]  # [1, hidden_dim]\n",
    "            \n",
    "            # Calculate cosine similarity with all documents\n",
    "            similarities = torch.nn.functional.cosine_similarity(\n",
    "                query_embedding.unsqueeze(0),  # [1, 1, hidden_dim]\n",
    "                doc_embeddings.unsqueeze(1),   # [num_docs, 1, hidden_dim]\n",
    "                dim=2\n",
    "            )\n",
    "            \n",
    "            return similarities.squeeze().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in bi-encoder scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_encoder_hybrid_model(corpus, top_k=50):\n",
    "    \"\"\"\n",
    "    Creates a hybrid model using BM25 for initial retrieval and Cross-Encoder for re-ranking\n",
    "    with memory optimization\n",
    "    \"\"\"\n",
    "    # Initialize BM25\n",
    "    tokenized_corpus = [doc.split() for doc in corpus if isinstance(doc, str)]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    # Initialize Cross-Encoder\n",
    "    model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    cross_encoder = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def get_hybrid_scores(query_text):\n",
    "        try:\n",
    "            # 1. Get initial candidates using BM25\n",
    "            query_tokens = query_text.split()\n",
    "            bm25_scores = np.array(bm25.get_scores(query_tokens))\n",
    "            top_k_idx = np.argsort(bm25_scores)[-top_k:]\n",
    "            \n",
    "            # 2. Process in smaller batches\n",
    "            batch_size = 10\n",
    "            final_scores = np.zeros(len(corpus))\n",
    "            \n",
    "            for i in range(0, len(top_k_idx), batch_size):\n",
    "                batch_idx = top_k_idx[i:i + batch_size]\n",
    "                pairs = [(query_text, corpus[idx]) for idx in batch_idx]\n",
    "                \n",
    "                features = tokenizer.batch_encode_plus(\n",
    "                    pairs,\n",
    "                    max_length=256,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = cross_encoder(**features)\n",
    "                    # Handle both single-label and multi-label cases\n",
    "                    if outputs.logits.shape[1] == 1:\n",
    "                        # For single-label case\n",
    "                        scores = torch.sigmoid(outputs.logits).squeeze(-1).numpy()\n",
    "                    else:\n",
    "                        # For multi-label case\n",
    "                        scores = softmax(outputs.logits, dim=1)[:, 1].numpy()\n",
    "                \n",
    "                final_scores[batch_idx] = scores\n",
    "                \n",
    "                # Clear CUDA cache if using GPU\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            return final_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hybrid scoring: {e}\")\n",
    "            print(f\"Shape of logits: {outputs.logits.shape if 'outputs' in locals() else 'unknown'}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_hybrid_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dual_encoder_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a dual encoder model using pre-trained sentence transformers\n",
    "    with mean pooling and attention mechanism for better semantic matching\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of text documents\n",
    "        batch_size: Batch size for processing documents\n",
    "    Returns:\n",
    "        scoring function that computes similarity between query and documents\n",
    "    \"\"\"\n",
    "    # Initialize model - using a different pre-trained model specialized for semantic search\n",
    "    model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    \n",
    "    # Pre-compute document embeddings in batches with progress bar\n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = model.encode(\n",
    "        corpus,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True\n",
    "    )\n",
    "    \n",
    "    # Move embeddings to CPU and normalize\n",
    "    doc_embeddings = doc_embeddings.cpu()\n",
    "    doc_embeddings = torch.nn.functional.normalize(doc_embeddings, p=2, dim=1)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Encode query\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "            \n",
    "            # Move to CPU and normalize\n",
    "            query_embedding = query_embedding.cpu()\n",
    "            query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=0)\n",
    "            \n",
    "            # Calculate cosine similarity efficiently\n",
    "            similarities = torch.matmul(\n",
    "                doc_embeddings, \n",
    "                query_embedding\n",
    "            )\n",
    "            \n",
    "            return similarities.numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in dual encoder scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of data for querying (e.g., 100 random samples)\n",
    "query_data = data.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe83256f29084738834b2b0b8d90cbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/791 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276eb7f5f49c4ef0855b2aea6f51c682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57b6ea2ff26479795a51a073b372043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae8f29f3d60472197e41ee236d22c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a0e47b248549aabbb9b49e97e38f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11020ecbc045407da279586a745aab7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/25.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf9e9a5b664417882ddaad2aa1bb2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08110edf10314d05993a250ea3f89db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65b81fe3c0748ae9ee44b96ef5d0d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O1.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45f6107dfe24ffc95d0220a8bcb3c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O2.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba7956807b2417aae2cffe714697d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O3.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed4ea8ad1c3406dbef6a4c8f44c3e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O4.onnx:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4d16959e794417a47b6ae4bac58535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_arm64.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2164fb87709e4279a8c76135c1bfaffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664abd25df3d41f58b5694a04b9a5e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512_vnni.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e92aabd32694bc080c0a41f1e721df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_quint8_avx2.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cd098e47154cc09d35f64daefb75d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.bin:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1c1006474b4d7aa5e74e418f07a82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino/openvino_model.xml:   0%|          | 0.00/212k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317a270cfab9415d986db40a4e7af6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.bin:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7325acdac700443aad253f541f4da6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)nvino/openvino_model_qint8_quantized.xml:   0%|          | 0.00/368k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99799ff57cac4119b9cf6a297d050972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9638cb960a9477fae3ed6f0020159e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765fb21dd80943829b76ca0564c51b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6341343994cb42c88a138c9d9b9ffa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4036da5a8254271a7bb27dfe478bf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f843a776bd472e81539461ea4a0f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c41bc0b64e94881a0fa7a61332e91d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851926693c3b4c1d8bce106a0a27b04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing document embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b244130e96bd4174a10bc9c0a04e5998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluated queries: 100\n",
      "MSE Average for dual_encoder model: 0.3694\n",
      "MSE Standard Deviation for dual_encoder model: 0.5199\n"
     ]
    }
   ],
   "source": [
    "dual_encoder = create_dual_encoder_model(data['processed_review'].tolist())\n",
    "dual_encoder_score = evaluate_model(dual_encoder, query_data, data, 'dual_encoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
