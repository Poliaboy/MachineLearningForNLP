{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TripAdvisor Recommendation System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import CrossEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device =  torch.device('mps') if torch.backends.mps.is_available() else device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (878561, 10)\n",
      "DataFrame shape after filtering: (436391, 10)\n",
      "   offering_id   service  cleanliness   overall     value  location  \\\n",
      "0        72572  4.601010     4.636364  4.388889  4.323232  4.570707   \n",
      "1        72579  4.232000     4.240000  3.888000  4.152000  4.192000   \n",
      "2        72586  4.250000     4.287879  4.045455  4.053030  4.537879   \n",
      "3        72598  3.243243     3.243243  2.918919  3.054054  3.027027   \n",
      "4        73236  4.277778     3.111111  3.388889  3.777778  4.111111   \n",
      "\n",
      "   sleep_quality     rooms                                            reviews  \n",
      "0       4.333333  4.282828  I had to make fast visit to seattle and I foun...  \n",
      "1       3.768000  3.856000  Great service, rooms were clean, could use som...  \n",
      "2       4.113636  3.992424  Beautiful views of the space needle - especial...  \n",
      "3       3.270270  3.189189  This hotel is in need of some serious updates....  \n",
      "4       3.722222  3.222222  My experience at this days inn was perfect. th...  \n",
      "\n",
      "DataFrame shape: (3754, 9)\n",
      "\n",
      "Column names: ['offering_id', 'service', 'cleanliness', 'overall', 'value', 'location', 'sleep_quality', 'rooms', 'reviews']\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "print(f\"Original DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Convert the 'ratings' column from string to dictionary\n",
    "df['ratings'] = df['ratings'].apply(ast.literal_eval)\n",
    "\n",
    "# Define required aspects\n",
    "required_aspects = [\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"]\n",
    "\n",
    "# Filter rows with at least the required aspects\n",
    "df_filtered = df[df['ratings'].apply(lambda x: all(aspect in x for aspect in required_aspects))]\n",
    "\n",
    "print(f\"DataFrame shape after filtering: {df_filtered.shape}\")\n",
    "\n",
    "if df_filtered.empty:\n",
    "    print(\"No reviews found with all required aspects. Printing unique aspects found in the dataset:\")\n",
    "    all_aspects = set()\n",
    "    for rating in df['ratings']:\n",
    "        all_aspects.update(rating.keys())\n",
    "    print(sorted(all_aspects))\n",
    "    data = pd.DataFrame(columns=['offering_id'] + required_aspects + ['reviews'])\n",
    "else:\n",
    "    # Group by offering_id\n",
    "    data = df_filtered.groupby('offering_id').agg({\n",
    "        'text': ' '.join,  # Concatenate all reviews\n",
    "        'ratings': list  # Keep all ratings\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calculate average ratings for each aspect\n",
    "    for aspect in required_aspects:\n",
    "        data[aspect] = data['ratings'].apply(lambda x: np.mean([review.get(aspect, np.nan) for review in x]))\n",
    "\n",
    "    # Rename 'text' column to 'reviews'\n",
    "    data = data.rename(columns={'text': 'reviews'})\n",
    "\n",
    "    # Select and order the final columns\n",
    "    final_columns = ['offering_id'] + required_aspects + ['reviews']\n",
    "    data = data[final_columns]\n",
    "\n",
    "# Print the first few rows and shape of the processed data\n",
    "print(data.head())\n",
    "print(\"\\nDataFrame shape:\", data.shape)\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"\\nColumn names:\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Enhanced text preprocessing function with multiple cleaning steps\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and handle basic cleaning\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove numbers and special characters, keeping only letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Remove short words (length < 4)\n",
    "    tokens = [token for token in tokens if len(token) > 3]\n",
    "    \n",
    "    # Lemmatization (convert words to their base form)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Remove common hotel-related words that might not add value\n",
    "    hotel_stopwords = {\n",
    "        'hotel', 'room', 'stay', 'stayed', 'night', 'day', \n",
    "        'would', 'could', 'really', 'get', 'got', 'one',\n",
    "        'also', 'us', 'back', 'even', 'well'\n",
    "    }\n",
    "    tokens = [token for token in tokens if token not in hotel_stopwords]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    processed_text = ' '.join(processed_text.split())\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply text preprocessing to reviews\n",
    "data['processed_reviews'] = data['reviews'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews Statistics:\n",
      "Average words per hotel: 16916.49\n",
      "Median words per hotel: 5509.50\n",
      "Min words: 1\n",
      "Max words: 294452\n",
      "\n",
      "Percentiles:\n",
      "count      3754.000000\n",
      "mean      16916.485882\n",
      "std       28293.752907\n",
      "min           1.000000\n",
      "25%        1682.750000\n",
      "50%        5509.500000\n",
      "75%       20139.500000\n",
      "max      294452.000000\n",
      "Name: reviews, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processed Reviews Statistics:\n",
      "Average words per hotel: 6976.72\n",
      "Median words per hotel: 2232.00\n",
      "Min words: 0\n",
      "Max words: 124229\n",
      "\n",
      "Percentiles:\n",
      "count      3754.000000\n",
      "mean       6976.722429\n",
      "std       11747.961900\n",
      "min           0.000000\n",
      "25%         674.250000\n",
      "50%        2232.000000\n",
      "75%        8372.500000\n",
      "max      124229.000000\n",
      "Name: processed_reviews, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate word counts\n",
    "reviews_word_counts = data['reviews'].str.split().str.len()\n",
    "processed_word_counts = data['processed_reviews'].str.split().str.len()\n",
    "\n",
    "print(\"Reviews Statistics:\")\n",
    "print(f\"Average words per hotel: {reviews_word_counts.mean():.2f}\")\n",
    "print(f\"Median words per hotel: {reviews_word_counts.median():.2f}\")\n",
    "print(f\"Min words: {reviews_word_counts.min()}\")\n",
    "print(f\"Max words: {reviews_word_counts.max()}\")\n",
    "print(f\"\\nPercentiles:\")\n",
    "print(reviews_word_counts.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Processed Reviews Statistics:\")\n",
    "print(f\"Average words per hotel: {processed_word_counts.mean():.2f}\")\n",
    "print(f\"Median words per hotel: {processed_word_counts.median():.2f}\")\n",
    "print(f\"Min words: {processed_word_counts.min()}\")\n",
    "print(f\"Max words: {processed_word_counts.max()}\")\n",
    "print(f\"\\nPercentiles:\")\n",
    "print(processed_word_counts.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, query_data, full_data):\n",
    "    mse_scores = []\n",
    "    aspects = [\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"]\n",
    "    total_scoring_time = 0\n",
    "    \n",
    "    for idx, query in tqdm(query_data.iterrows(), total=len(query_data), desc=\"Evaluating queries\"):\n",
    "        try:\n",
    "            # Time the scoring\n",
    "            start_time = time.time()\n",
    "            scores = model(query['processed_reviews'])\n",
    "            scoring_time = time.time() - start_time\n",
    "            total_scoring_time += scoring_time\n",
    "            \n",
    "            # Find best matching document\n",
    "            scores[full_data['offering_id'] == query['offering_id']] = 0  # Exclude the query document\n",
    "            best_index = np.argmax(scores)\n",
    "            \n",
    "            # Calculate MSE\n",
    "            query_ratings = query[aspects].values\n",
    "            best_doc_ratings = full_data.iloc[best_index][aspects].values\n",
    "            mse = mean_squared_error(query_ratings, best_doc_ratings)\n",
    "            mse_scores.append(mse)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {idx}\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            print(f\"Query text sample: {str(query['processed_review'])[:100]}\")\n",
    "            continue\n",
    "\n",
    "    # Print summary statistics\n",
    "    avg_scoring_time = total_scoring_time / len(query_data) if query_data.shape[0] > 0 else 0\n",
    "    print(f\"\\nAverage scoring time per query: {avg_scoring_time:.4f} seconds\")\n",
    "    print(f\"Total scoring time: {total_scoring_time:.4f} seconds\")\n",
    "    print(f\"Successfully evaluated queries: {len(mse_scores)}/{len(query_data)}\")\n",
    "    \n",
    "    if not mse_scores:\n",
    "        print(\"Warning: No valid evaluations were performed\")\n",
    "        return float('inf')\n",
    "    \n",
    "    avg_mse = np.mean(mse_scores)\n",
    "    print(f\"MSE Average: {avg_mse:.4f}\")\n",
    "    \n",
    "    return avg_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_bm25(model, query_data, full_data, column_name='processed_reviews'):\n",
    "    mse_scores = []\n",
    "    aspects = [\"service\", \"cleanliness\", \"overall\", \"value\", \"location\", \"sleep_quality\", \"rooms\"]\n",
    "    total_scoring_time = 0\n",
    "    \n",
    "    for idx, query in tqdm(query_data.iterrows(), total=len(query_data), desc=\"Evaluating queries\"):\n",
    "        try:\n",
    "            # Ensure the query text is properly formatted\n",
    "            query_text = query[column_name]\n",
    "            \n",
    "            # Split the query text into tokens\n",
    "            query_tokens = query_text.split(' ')\n",
    "            \n",
    "            # Time the scoring\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Get scores\n",
    "            try:\n",
    "                scores = model.get_scores(query_tokens)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during scoring for query {idx}: {str(e)}\")\n",
    "                print(f\"Query tokens: {query_tokens[:10]}...\")  # Print first 10 tokens\n",
    "                continue\n",
    "                \n",
    "            scoring_time = time.time() - start_time\n",
    "            total_scoring_time += scoring_time\n",
    "            \n",
    "            # Find best matching document\n",
    "            scores[full_data['offering_id'] == query['offering_id']] = -float('inf')  # Exclude the query document\n",
    "            best_index = np.argmax(scores)\n",
    "            \n",
    "            # Calculate MSE\n",
    "            query_ratings = query[aspects].values\n",
    "            best_doc_ratings = full_data.iloc[best_index][aspects].values\n",
    "            mse = mean_squared_error(query_ratings, best_doc_ratings)\n",
    "            mse_scores.append(mse)\n",
    "            \n",
    "\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {idx}\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            print(f\"Query text sample: {str(query['processed_review'])[:100]}\")\n",
    "            continue\n",
    "\n",
    "    # Print summary statistics\n",
    "    avg_scoring_time = total_scoring_time / len(query_data) if query_data.shape[0] > 0 else 0\n",
    "    print(f\"\\nAverage scoring time per query: {avg_scoring_time:.4f} seconds\")\n",
    "    print(f\"Total scoring time: {total_scoring_time:.4f} seconds\")\n",
    "    print(f\"Successfully evaluated queries: {len(mse_scores)}/{len(query_data)}\")\n",
    "    \n",
    "    if not mse_scores:\n",
    "        print(\"Warning: No valid evaluations were performed\")\n",
    "        return float('inf')\n",
    "    \n",
    "    avg_mse = np.mean(mse_scores)\n",
    "    print(f\"MSE Average: {avg_mse:.4f}\")\n",
    "    \n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition and eval function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_bm25_model(corpus):\n",
    "    try:\n",
    "        tokenized_corpus = [doc.split(' ') for doc in corpus if isinstance(doc, str)]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "        return bm25\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating BM25 model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_model_with_dense_retriever(corpus, alpha=0.6, beta=0.4):\n",
    "    \"\"\"\n",
    "    Creates a hybrid model combining BM25 and SentenceTransformer with GPU support\n",
    "    \"\"\"\n",
    "    # Initialize models\n",
    "    tokenized_corpus = [doc.split() for doc in corpus if isinstance(doc, str)]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    # Use a model specifically trained for semantic similarity\n",
    "    dense_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1').to(device)\n",
    "    \n",
    "    # Pre-compute document embeddings with normalization\n",
    "    doc_embeddings = dense_model.encode(\n",
    "        corpus,\n",
    "        convert_to_tensor=True,\n",
    "        device=device,\n",
    "        normalize_embeddings=True,  # Important for proper similarity calculation\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    def get_hybrid_scores(query_text):\n",
    "        try:\n",
    "            # 1. Get BM25 scores\n",
    "            query_tokens = query_text.split()\n",
    "            bm25_scores = np.array(bm25.get_scores(query_tokens))\n",
    "            \n",
    "            # Normalize BM25 scores to [0,1] range\n",
    "            if bm25_scores.max() != bm25_scores.min():\n",
    "                bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())\n",
    "            \n",
    "            # 2. Get dense retriever scores\n",
    "            query_embedding = dense_model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            # Calculate similarity scores\n",
    "            dense_scores = torch.matmul(doc_embeddings, query_embedding).cpu().numpy()\n",
    "            \n",
    "            # Normalize dense scores to [0,1] range\n",
    "            if dense_scores.max() != dense_scores.min():\n",
    "                dense_scores = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min())\n",
    "            \n",
    "            # Combine scores with weights\n",
    "            final_scores = alpha * bm25_scores + beta * dense_scores\n",
    "            \n",
    "            return final_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hybrid scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_hybrid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colbert_model(corpus, max_length=128):\n",
    "    \"\"\"\n",
    "    Creates an improved ColBERT model with better token-level interaction\n",
    "    \"\"\"\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    # Pre-compute document embeddings with attention mask\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for doc in tqdm(corpus, desc=\"Computing document embeddings\"):\n",
    "        tokens = tokenizer(\n",
    "            doc,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            # Use attention mask to get valid token embeddings\n",
    "            mask = tokens['attention_mask'].unsqueeze(-1)\n",
    "            embeddings = outputs.last_hidden_state * mask\n",
    "            # Normalize embeddings\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)\n",
    "            doc_embeddings.append(embeddings.cpu())\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            query_tokens = tokenizer(\n",
    "                query_text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                query_outputs = model(**query_tokens)\n",
    "                query_mask = query_tokens['attention_mask'].unsqueeze(-1)\n",
    "                query_embeddings = query_outputs.last_hidden_state * query_mask\n",
    "                query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=-1)\n",
    "            \n",
    "            scores = []\n",
    "            query_embeddings = query_embeddings.cpu()\n",
    "            \n",
    "            for doc_emb in doc_embeddings:\n",
    "                # MaxSim operation with proper masking\n",
    "                sim_matrix = torch.matmul(query_embeddings.squeeze(), doc_emb.squeeze().transpose(-1, -2))\n",
    "                score = sim_matrix.max(dim=1)[0].mean().item()\n",
    "                scores.append(score)\n",
    "            \n",
    "            return np.array(scores)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ColBERT scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_biencoder_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates an improved bi-encoder model with better batch processing\n",
    "    and normalization\n",
    "    \"\"\"\n",
    "    model_name = 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    # Pre-compute document embeddings with proper pooling\n",
    "    doc_embeddings_list = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Computing document embeddings\"):\n",
    "        batch = corpus[i:i + batch_size]\n",
    "        \n",
    "        encoded = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # Mean pooling with attention mask\n",
    "            attention_mask = encoded['attention_mask'].unsqueeze(-1)\n",
    "            token_embeddings = outputs.last_hidden_state * attention_mask\n",
    "            sentence_embeddings = token_embeddings.sum(1) / attention_mask.sum(1)\n",
    "            # Normalize embeddings\n",
    "            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            doc_embeddings_list.append(sentence_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings_list, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            query_encoded = tokenizer(\n",
    "                query_text,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                query_outputs = model(**query_encoded)\n",
    "                # Mean pooling for query\n",
    "                attention_mask = query_encoded['attention_mask'].unsqueeze(-1)\n",
    "                token_embeddings = query_outputs.last_hidden_state * attention_mask\n",
    "                query_embedding = token_embeddings.sum(1) / attention_mask.sum(1)\n",
    "                query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "            \n",
    "            # Compute similarity scores\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding.cpu().transpose(0, 1))\n",
    "            \n",
    "            return similarities.squeeze().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in bi-encoder scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dual_encoder_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a dual encoder model using pre-trained sentence transformers\n",
    "    with improved semantic matching capabilities\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of text documents\n",
    "        batch_size: Batch size for processing documents\n",
    "    Returns:\n",
    "        scoring function that computes similarity between query and documents\n",
    "    \"\"\"\n",
    "        # Initialize model with a strong semantic search model\n",
    "    model = SentenceTransformer('multi-qa-mpnet-base-dot-v1').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    # Pre-compute document embeddings with proper batching and normalization\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Encode batch\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,  # Important for cosine similarity\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Encode query with same normalization\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            # Move query embedding to same device as documents\n",
    "            query_embedding = query_embedding.cpu()\n",
    "            \n",
    "            # Compute dot product similarity\n",
    "            # Using dot product since embeddings are normalized\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            scores = similarities.numpy()\n",
    "            \n",
    "            # Ensure scores are properly scaled\n",
    "            scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in dual encoder scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_hybrid_model_with_dense_retriever(corpus, alpha=0.85, beta=0.15):\n",
    "    \"\"\"\n",
    "    Creates a hybrid model combining BM25 and SentenceTransformer with GPU support\n",
    "    \"\"\"\n",
    "    # Initialize models\n",
    "    tokenized_corpus = [doc.split() for doc in corpus if isinstance(doc, str)]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    dense_model = SentenceTransformer('all-mpnet-base-v2').to(device)\n",
    "    \n",
    "    # Pre-compute document embeddings\n",
    "    doc_embeddings = dense_model.encode(corpus, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    def get_hybrid_scores(query_text):\n",
    "        try:\n",
    "            # 1. Get BM25 scores\n",
    "            query_tokens = query_text.split()\n",
    "            bm25_scores = np.array(bm25.get_scores(query_tokens))\n",
    "                    \n",
    "            # 2. Get dense retriever scores\n",
    "            query_embedding = dense_model.encode(query_text, convert_to_tensor=True, device=device)\n",
    "            \n",
    "            # Calculate cosine similarity on GPU\n",
    "            dense_scores = torch.nn.functional.cosine_similarity(\n",
    "                query_embedding.unsqueeze(0),\n",
    "                doc_embeddings\n",
    "            ).cpu().numpy()\n",
    "              \n",
    "            # Combine scores\n",
    "            final_scores = alpha * bm25_scores + beta * dense_scores\n",
    "            \n",
    "            return final_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hybrid scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_hybrid_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dual_encoder_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a dual encoder model using pre-trained sentence transformers\n",
    "    with explicit GPU handling\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = model.encode(\n",
    "        corpus,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Normalize embeddings on GPU\n",
    "    doc_embeddings = torch.nn.functional.normalize(doc_embeddings, p=2, dim=1)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=0)\n",
    "            \n",
    "            # Calculate similarities on GPU\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            \n",
    "            return similarities.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in dual encoder scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_aware_semantic_model(corpus, n_topics=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a model that combines topic modeling with semantic embeddings\n",
    "    to capture both thematic and semantic similarity\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize models\n",
    "    count_vec = CountVectorizer(max_features=5000, stop_words='english')\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    semantic_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "    \n",
    "    # Fit topic model\n",
    "    print(\"Computing topic distributions...\")\n",
    "    doc_term_matrix = count_vec.fit_transform(corpus)\n",
    "    topic_distributions = lda.fit_transform(doc_term_matrix)\n",
    "    \n",
    "    # Pre-compute semantic embeddings\n",
    "    print(\"Computing semantic embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Get semantic embeddings\n",
    "        batch_embeddings = semantic_model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Get topic distributions for this batch\n",
    "        batch_topics = topic_distributions[i:i + len(batch_texts)]\n",
    "        \n",
    "        # Combine semantic and topic information\n",
    "        enhanced_embeddings = []\n",
    "        for j, emb in enumerate(batch_embeddings):\n",
    "            # Get topic distribution\n",
    "            topic_dist = batch_topics[j]\n",
    "            topic_entropy = -np.sum(topic_dist * np.log2(topic_dist + 1e-10))\n",
    "            \n",
    "            # Weight embedding based on topic clarity\n",
    "            topic_weight = 1 + (1 - topic_entropy/np.log2(n_topics))\n",
    "            enhanced_emb = emb * topic_weight\n",
    "            enhanced_embeddings.append(enhanced_emb)\n",
    "        \n",
    "        # Stack and normalize\n",
    "        batch_enhanced = torch.stack(enhanced_embeddings)\n",
    "        batch_enhanced = torch.nn.functional.normalize(batch_enhanced, p=2, dim=1)\n",
    "        doc_embeddings.append(batch_enhanced.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Get semantic embedding for query\n",
    "            query_embedding = semantic_model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            # Get topic distribution for query\n",
    "            query_bow = count_vec.transform([query_text])\n",
    "            query_topics = lda.transform(query_bow)[0]\n",
    "            \n",
    "            # Weight query embedding based on topic clarity\n",
    "            topic_entropy = -np.sum(query_topics * np.log2(query_topics + 1e-10))\n",
    "            topic_weight = 1 + (1 - topic_entropy/np.log2(n_topics))\n",
    "            query_embedding = query_embedding * topic_weight\n",
    "            \n",
    "            # Normalize and compute similarity\n",
    "            query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=0)\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding.cpu())\n",
    "            scores = similarities.numpy()\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in topic-aware scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mpnet_domain_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a model using MPNet with domain adaptation for review similarity\n",
    "    Uses all-mpnet-base-v2 which is the best performing general purpose model\n",
    "    according to Sentence-Transformers documentation\n",
    "    \"\"\"\n",
    "    # Initialize model with the best performing general purpose model\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Encode with special handling for reviews\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            # Move query embedding to CPU\n",
    "            query_embedding = query_embedding.cpu()\n",
    "            \n",
    "            # Compute similarity using dot product (since vectors are normalized)\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            \n",
    "            return similarities.numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiqa_mpnet_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Uses multi-qa-mpnet-base-dot-v1 which achieves 57.60 performance on semantic search\n",
    "    according to Sentence-Transformers documentation\n",
    "    \"\"\"\n",
    "    # Initialize with the best performing semantic search model\n",
    "    model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            query_embedding = query_embedding.cpu()\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            \n",
    "            # Scale scores to [0,1]\n",
    "            scores = similarities.numpy()\n",
    "            scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simcse_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a model using unsupervised SimCSE which has shown SOTA performance\n",
    "    on semantic similarity tasks\n",
    "    \"\"\"\n",
    "    # Initialize with unsupervised SimCSE model\n",
    "    model = SentenceTransformer('princeton-nlp/unsup-simcse-bert-base-uncased').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Get embeddings with temperature scaling\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Get query embedding with same temperature scaling\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            query_embedding = query_embedding.cpu()\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            scores = similarities.numpy()\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bge_v15_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a model using bge-base-en-v1.5, one of the strongest embedding models\n",
    "    currently available\n",
    "    \"\"\"\n",
    "    # Initialize with BGE-M3 model\n",
    "    model = SentenceTransformer('BAAI/bge-large-en-v1.5').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Add instruction prefix for better retrieval performance\n",
    "        batch_texts = [f\"Represent this hotel review for retrieval: {text}\" for text in batch_texts]\n",
    "        \n",
    "        # Get embeddings\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Add instruction prefix for query\n",
    "            query_text = f\"Represent this hotel review for retrieval: {query_text}\"\n",
    "            \n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            query_embedding = query_embedding.cpu()\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            scores = similarities.numpy()\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in BGE-M3 scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uae_model(corpus, batch_size=16):\n",
    "    \"\"\"\n",
    "    Creates a model using WhereIsAI/UAE-Large-V1, a powerful embedding model\n",
    "    specifically designed for universal text embeddings\n",
    "    \"\"\"\n",
    "    # Initialize UAE model\n",
    "    model = SentenceTransformer('WhereIsAI/UAE-Large-V1').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Get embeddings with special handling for long texts\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Get query embedding\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            query_embedding = query_embedding.cpu()\n",
    "            \n",
    "            # Compute similarity using dot product (vectors are normalized)\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            scores = similarities.numpy()\n",
    "            \n",
    "            # Apply temperature scaling for better discrimination\n",
    "            temperature = 0.05\n",
    "            scores = np.exp(scores / temperature)\n",
    "            scores = scores / np.sum(scores)\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in UAE scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mxbai_embed_model(corpus, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a model using mixedbread-ai/mxbai-embed-large-v1, which achieves SOTA \n",
    "    performance on MTEB benchmark\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    model = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1').to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Add instruction prefix for better retrieval performance\n",
    "        batch_texts = [f\"Represent this sentence for searching relevant passages: {text}\" for text in batch_texts]\n",
    "        \n",
    "        # Get embeddings\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        doc_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Add instruction prefix for query\n",
    "            query_text = f\"Represent this sentence for searching relevant passages: {query_text}\"\n",
    "            \n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                device=device,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            \n",
    "            query_embedding = query_embedding.cpu()\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding)\n",
    "            scores = similarities.numpy()\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mxbai-embed scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mxbai_colbert_model(corpus, batch_size=16):\n",
    "    \"\"\"\n",
    "    Creates a model using mixedbread-ai/mxbai-colbert-large-v1 using direct \n",
    "    sentence-transformers implementation with proper pooling\n",
    "    \"\"\"\n",
    "    # Initialize model and tokenizer\n",
    "    model_name = 'mixedbread-ai/mxbai-colbert-large-v1'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    print(\"Computing document embeddings...\")\n",
    "    doc_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus), batch_size), desc=\"Processing documents\"):\n",
    "        batch_texts = corpus[i:i + batch_size]\n",
    "        \n",
    "        # Add instruction prefix for better retrieval\n",
    "        batch_texts = [f\"Represent this document for retrieval: {text}\" for text in batch_texts]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # Mean pooling\n",
    "            attention_mask = encoded['attention_mask'].unsqueeze(-1)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            sentence_embeddings = (token_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "            # Normalize\n",
    "            sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "            doc_embeddings.append(sentence_embeddings.cpu())\n",
    "    \n",
    "    doc_embeddings = torch.cat(doc_embeddings, dim=0)\n",
    "    \n",
    "    def get_scores(query_text):\n",
    "        try:\n",
    "            # Add instruction prefix\n",
    "            query_text = f\"Represent this query for retrieval: {query_text}\"\n",
    "            \n",
    "            # Encode query\n",
    "            encoded = tokenizer(\n",
    "                query_text,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get query embedding\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**encoded)\n",
    "                attention_mask = encoded['attention_mask'].unsqueeze(-1)\n",
    "                token_embeddings = outputs.last_hidden_state\n",
    "                query_embedding = (token_embeddings * attention_mask).sum(1) / attention_mask.sum(1)\n",
    "                query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "            \n",
    "            # Move to CPU and compute similarity\n",
    "            query_embedding = query_embedding.cpu()\n",
    "            similarities = torch.matmul(doc_embeddings, query_embedding.T)\n",
    "            scores = similarities.squeeze().numpy()\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            temperature = 0.05\n",
    "            scores = np.exp(scores / temperature)\n",
    "            scores = scores / np.sum(scores)\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mxbai-colbert scoring: {e}\")\n",
    "            return np.zeros(len(corpus))\n",
    "    \n",
    "    return get_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of data for querying (e.g., 100 random samples)\n",
    "query_data = data.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5626555533d4733ba2a43e25569179e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56a5477bbb8446e92a4228a24795bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5eb6507115a4251a5bebdb473169110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f43786d8bb4e1e9e159ab7c6734ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36094c79770843588eabd1209891bf0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70295075a5e34bcd9d11d56baeb0c6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664fafb269ba4de18511ab5290b0bf5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30aece57626c4b6eaea7acb05afa6822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc481c5c32d04350968cc439847bc03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O1.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf66ca49d1a46c3ac0c5f9d2148be86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O2.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481a5b722df944eaa965ac1b014af288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O3.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dca98da67441e5a6308e7e33c42f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O4.onnx:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2521cea48a34aae9693d778317994dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_arm64.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfeb32af8fe34f9fa51f8b62c57e537c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c8dc2ee6044f7ead042129b954fb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512_vnni.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadb54de24c44bc0b3def559e5234abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_quint8_avx2.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d644680d5942ff8e5419289fc756ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b20a8be592f405d9eafe5c4b238c10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino/openvino_model.xml:   0%|          | 0.00/433k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2f28ffc7504c188a20d6db6102251a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.bin:   0%|          | 0.00/110M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bc975274f44d03b75ac24e23240481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)nvino/openvino_model_qint8_quantized.xml:   0%|          | 0.00/742k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7fe3155b67483db2cb93cb7308170c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098ef5121a8845d78491e3e8a4e359d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1060f2b56b45bd83bcfb4ea5b6c6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55a3e7c111a4ff88a38141d38df265b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db784cae2b35423297463bba282ce5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61a0522cca24e5c978c461898209a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5defaf814c1745f598e6dfc129542deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c02536188d4c1bb53c1eae938ba11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hybrid_model = create_hybrid_model_with_dense_retriever(data['processed_reviews'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 100/100 [21:24<00:00, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scoring time per query: 12.8443 seconds\n",
      "Total scoring time: 1284.4333 seconds\n",
      "Successfully evaluated queries: 100/100\n",
      "MSE Average: 0.4910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hybrid_scores = evaluate_model(hybrid_model, query_data, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
