{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1 length: 2700\n",
      "Article 2 length: 2698\n",
      "Article 3 length: 4539\n"
     ]
    }
   ],
   "source": [
    "article1 = open('article1.txt', 'r').read()\n",
    "article2 = open('article2.txt', 'r').read()\n",
    "article3 = open('article3.txt', 'r').read()\n",
    "\n",
    "print(\"Article 1 length:\", len(article1))\n",
    "print(\"Article 2 length:\", len(article2))\n",
    "print(\"Article 3 length:\", len(article3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words in Article 1: 263\n",
      "Number of words in Article 2: 255\n",
      "Number of words in Article 3: 404\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Lemmatize and filter words\n",
    "    return [lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Preprocess articles\n",
    "preprocessed_article1 = preprocess(article1)\n",
    "preprocessed_article2 = preprocess(article2)\n",
    "preprocessed_article3 = preprocess(article3)\n",
    "\n",
    "print(\"\\nNumber of words in Article 1:\", len(preprocessed_article1))\n",
    "print(\"Number of words in Article 2:\", len(preprocessed_article2))\n",
    "print(\"Number of words in Article 3:\", len(preprocessed_article3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article similarity results:\n",
      "Most similar articles: 1 and 2\n",
      "Similarity score: 0.3592\n",
      "Least similar articles: 2 and 3\n",
      "Similarity score: 0.0115\n",
      "\n",
      "Similarity matrix:\n",
      "1.0000\t0.3592\t0.0159\t\n",
      "0.3592\t1.0000\t0.0115\t\n",
      "0.0159\t0.0115\t1.0000\t\n"
     ]
    }
   ],
   "source": [
    "# Convert preprocessed word lists to strings for TfidfVectorizer\n",
    "article1_text = ' '.join(preprocessed_article1)\n",
    "article2_text = ' '.join(preprocessed_article2)\n",
    "article3_text = ' '.join(preprocessed_article3)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the articles\n",
    "tfidf_matrix = vectorizer.fit_transform([article1_text, article2_text, article3_text])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Find the most and least similar article pairs\n",
    "article_pairs = [(0, 1), (0, 2), (1, 2)]\n",
    "similarities = [cosine_similarities[i, j] for i, j in article_pairs]\n",
    "\n",
    "most_similar_pair = article_pairs[similarities.index(max(similarities))]\n",
    "least_similar_pair = article_pairs[similarities.index(min(similarities))]\n",
    "\n",
    "print(\"\\nArticle similarity results:\")\n",
    "print(f\"Most similar articles: {most_similar_pair[0] + 1} and {most_similar_pair[1] + 1}\")\n",
    "print(f\"Similarity score: {max(similarities):.4f}\")\n",
    "print(f\"Least similar articles: {least_similar_pair[0] + 1} and {least_similar_pair[1] + 1}\")\n",
    "print(f\"Similarity score: {min(similarities):.4f}\")\n",
    "\n",
    "# Print similarity matrix\n",
    "print(\"\\nSimilarity matrix:\")\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print(f\"{cosine_similarities[i, j]:.4f}\", end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two most similar sentences from articles 1 and 2:\n",
      "Sentence 1 (from article 1): Aerial footage shows Asheville, North Carolina before and after Helene's devastation\n",
      "\n",
      "Aerial footage is capturing the extent of Hurricane Helene's cataclysmic impact on Asheville, North Carolina.\n",
      "\n",
      "Sentence 2 (from article 2): Before-and-after images show Helene wiped parts of North Carolina off the map\n",
      "\n",
      "\n",
      "A river now flows where a North Carolina home and road once stood before Hurricane Helene.\n",
      "Similarity score: 0.2903\n"
     ]
    }
   ],
   "source": [
    "# Use the most similar pair of articles found earlier\n",
    "article_a = globals()[f\"article{most_similar_pair[0] + 1}\"]\n",
    "article_b = globals()[f\"article{most_similar_pair[1] + 1}\"]\n",
    "\n",
    "# Combine sentences from the two most similar articles\n",
    "same_topic_sentences = [preprocess(sent) for sent in sent_tokenize(article_a) + sent_tokenize(article_b)]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentences\n",
    "tfidf_matrix = vectorizer.fit_transform([' '.join(sent) for sent in same_topic_sentences])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "np.fill_diagonal(cosine_similarities, 0)  # Exclude self-similarity\n",
    "\n",
    "# Find the most similar sentence pair with one sentence from each article\n",
    "max_similarity = 0\n",
    "max_similarity_indices = (-1, -1)\n",
    "len_article_a = len(sent_tokenize(article_a))\n",
    "for i in range(len_article_a):\n",
    "    for j in range(len_article_a, len(same_topic_sentences)):\n",
    "        if cosine_similarities[i, j] > max_similarity:\n",
    "            max_similarity = cosine_similarities[i, j]\n",
    "            max_similarity_indices = (i, j)\n",
    "\n",
    "# Get the original sentences\n",
    "sentence1 = sent_tokenize(article_a)[max_similarity_indices[0]]\n",
    "sentence2 = sent_tokenize(article_b)[max_similarity_indices[1] - len_article_a]\n",
    "\n",
    "print(f\"Two most similar sentences from articles {most_similar_pair[0] + 1} and {most_similar_pair[1] + 1}:\")\n",
    "print(f\"Sentence 1 (from article {most_similar_pair[0] + 1}): {sentence1}\\n\")\n",
    "print(f\"Sentence 2 (from article {most_similar_pair[1] + 1}): {sentence2}\")\n",
    "print(f\"Similarity score: {max_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Two least similar sentences from article 2 and article 3:\n",
      "Sentence 1 (from article 2): Aerial footage shows Asheville, North Carolina before and after Helene's devastation\n",
      "\n",
      "Aerial footage is capturing the extent of Hurricane Helene's cataclysmic impact on Asheville, North Carolina.\n",
      "\n",
      "Sentence 2 (from article 3): This could translate to meaningful cost savings for high-volume enterprise deployments, as businesses may achieve better performance on specific tasks with fewer compute resources.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "article_a = globals()[f\"article{least_similar_pair[0] + 1}\"]\n",
    "article_b = globals()[f\"article{least_similar_pair[1] + 1}\"]\n",
    "# Demonstrate difference between articles on different topics\n",
    "different_topic_sentences = [preprocess(sent) for sent in sent_tokenize(article_a) + sent_tokenize(article_b)]\n",
    "\n",
    "# Create new TF-IDF vectorizer for different topics\n",
    "vectorizer_diff = TfidfVectorizer(min_df=1, stop_words=None)\n",
    "\n",
    "# Fit and transform the sentences\n",
    "tfidf_matrix_diff = vectorizer_diff.fit_transform([' '.join(sent) for sent in different_topic_sentences])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarities_diff = cosine_similarity(tfidf_matrix_diff, tfidf_matrix_diff)\n",
    "\n",
    "# Find the least similar sentence pair with one sentence from each article\n",
    "min_similarity = 1\n",
    "min_similarity_indices = (-1, -1)\n",
    "for i in range(len(sent_tokenize(article1))):\n",
    "    for j in range(len(sent_tokenize(article1)), len(different_topic_sentences)):\n",
    "        if cosine_similarities_diff[i, j] < min_similarity:\n",
    "            min_similarity = cosine_similarities_diff[i, j]\n",
    "            min_similarity_indices = (i, j)\n",
    "\n",
    "# Get the original sentences\n",
    "sentence_diff1 = sent_tokenize(article1)[min_similarity_indices[0]]\n",
    "sentence_diff2 = sent_tokenize(article3)[min_similarity_indices[1] - len(sent_tokenize(article1))]\n",
    "\n",
    "print(f\"\\nTwo least similar sentences from article {least_similar_pair[0] + 1} and article {least_similar_pair[1] + 1}:\")\n",
    "print(f\"Sentence 1 (from article {least_similar_pair[0] + 1}): {sentence_diff1}\\n\")\n",
    "print(f\"Sentence 2 (from article {least_similar_pair[1] + 1}): {sentence_diff2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
