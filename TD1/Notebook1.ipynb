{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alexs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1 length: 2700\n",
      "Article 2 length: 2698\n",
      "Article 3 length: 4539\n"
     ]
    }
   ],
   "source": [
    "article1 = open('article1.txt', 'r').read()\n",
    "article2 = open('article2.txt', 'r').read()\n",
    "article3 = open('article3.txt', 'r').read()\n",
    "\n",
    "print(\"Article 1 length:\", len(article1))\n",
    "print(\"Article 2 length:\", len(article2))\n",
    "print(\"Article 3 length:\", len(article3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of sentences in Article 1: 16\n",
      "Number of sentences in Article 2: 22\n",
      "Number of sentences in Article 3: 27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess function with lemmatization\n",
    "def preprocess(text):\n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Lemmatize and filter words\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words])\n",
    "\n",
    "# Preprocess articles\n",
    "preprocessed_article1 = [preprocess(sent) for sent in sent_tokenize(article1) if sent.strip()]\n",
    "preprocessed_article2 = [preprocess(sent) for sent in sent_tokenize(article2) if sent.strip()]\n",
    "preprocessed_article3 = [preprocess(sent) for sent in sent_tokenize(article3) if sent.strip()]\n",
    "\n",
    "print(\"\\nNumber of sentences in Article 1:\", len(preprocessed_article1))\n",
    "print(\"Number of sentences in Article 2:\", len(preprocessed_article2))\n",
    "print(\"Number of sentences in Article 3:\", len(preprocessed_article3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two most similar sentences from articles with the same topic:\n",
      "Sentence 1 (from article 1): Aerial footage shows Asheville, North Carolina before and after Helene's devastation\n",
      "\n",
      "Aerial footage is capturing the extent of Hurricane Helene's cataclysmic impact on Asheville, North Carolina.\n",
      "\n",
      "Sentence 2 (from article 2): Before-and-after images show Helene wiped parts of North Carolina off the map\n",
      "\n",
      "\n",
      "A river now flows where a North Carolina home and road once stood before Hurricane Helene.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine sentences from articles 1 and 2 (same topic)\n",
    "same_topic_sentences = preprocessed_article1 + preprocessed_article2\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentences\n",
    "tfidf_matrix = vectorizer.fit_transform(same_topic_sentences)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "np.fill_diagonal(cosine_similarities, 0)  # Exclude self-similarity\n",
    "\n",
    "\n",
    "# Find the most similar sentence pair with one sentence from each article\n",
    "max_similarity = 0\n",
    "max_similarity_indices = (-1, -1)\n",
    "for i in range(len(preprocessed_article1)):\n",
    "    for j in range(len(preprocessed_article1), len(same_topic_sentences)):\n",
    "        if cosine_similarities[i, j] > max_similarity:\n",
    "            max_similarity = cosine_similarities[i, j]\n",
    "            max_similarity_indices = (i, j)\n",
    "\n",
    "\n",
    "# Get the original sentences\n",
    "sentence1 = sent_tokenize(article1)[max_similarity_indices[0]]\n",
    "sentence2 = sent_tokenize(article2)[max_similarity_indices[1] - len(preprocessed_article1)]\n",
    "\n",
    "print(\"Two most similar sentences from articles with the same topic:\")\n",
    "print(f\"Sentence 1 (from article 1): {sentence1}\\n\")\n",
    "print(f\"Sentence 2 (from article 2): {sentence2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Two least similar sentences from articles with different topics:\n",
      "Sentence 1 (from article 1): Aerial footage shows Asheville, North Carolina before and after Helene's devastation\n",
      "\n",
      "Aerial footage is capturing the extent of Hurricane Helene's cataclysmic impact on Asheville, North Carolina.\n",
      "\n",
      "Sentence 2 (from article 3): Cohere just made it way easier for companies to create their own AI language models\n",
      "\n",
      "Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Demonstrate difference between articles on different topics\n",
    "different_topic_sentences = preprocessed_article1 + preprocessed_article3\n",
    "\n",
    "# Create new TF-IDF vectorizer for different topics\n",
    "vectorizer_diff = TfidfVectorizer(min_df=1, stop_words=None)\n",
    "\n",
    "# Fit and transform the sentences\n",
    "tfidf_matrix_diff = vectorizer_diff.fit_transform(different_topic_sentences)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarities_diff = cosine_similarity(tfidf_matrix_diff, tfidf_matrix_diff)\n",
    "\n",
    "# Find the least similar sentence pair with one sentence from each article\n",
    "min_similarity = 1\n",
    "min_similarity_indices = (-1, -1)\n",
    "for i in range(len(preprocessed_article1)):\n",
    "    for j in range(len(preprocessed_article1), len(different_topic_sentences)):\n",
    "        if cosine_similarities_diff[i, j] < min_similarity:\n",
    "            min_similarity = cosine_similarities_diff[i, j]\n",
    "            min_similarity_indices = (i, j)\n",
    "\n",
    "# Get the original sentences\n",
    "sentence_diff1 = sent_tokenize(article1)[min_similarity_indices[0]]\n",
    "sentence_diff2 = sent_tokenize(article3)[min_similarity_indices[1] - len(preprocessed_article1)]\n",
    "\n",
    "print(\"\\nTwo least similar sentences from articles with different topics:\")\n",
    "print(f\"Sentence 1 (from article 1): {sentence_diff1}\\n\")\n",
    "print(f\"Sentence 2 (from article 3): {sentence_diff2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
